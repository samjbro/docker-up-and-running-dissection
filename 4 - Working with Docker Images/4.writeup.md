# Chapter 4 - Working with Docker Images

Every Docker container requires a Docker image, which provides the blueprint for the container's contents.
You can either create your own Dockerfile, or download an image from a registry. A Docker image is made up 
of individual file system layers, corresponding to the individual commands found in the Dockerfile.

Docker uses a __storage backend__ in concert with the underlying Linux filesystem to build and manage the 
filesystem layers that it can then combine into a single usable image. The primary supported storage backends
include __AUFS,__ __BTRFS,__ __Device-mapper__ and __overlayfs,__ each of which provide a fast __copy-on-write
(CoW)__ system for image management.

## Anatomy of a Dockerfile

The Dockerfile is the recipe for your image, and contains all of the steps required to turn a clean Linux
distribution instance into the specific environment your app requires. The Dockerfile will typically be
located in the root directory of your application's source code.

See '3.appendix1.Dockerfile' for an example of a Dockerfile.

Remember that each instruction in a Dockerfile creates a new filesystem layer, which Docker stores; this allows
Docker to reuse these layers when other Dockerfiles contain an identical series of instructions. Docker will
always look for an image that contains as many of the same layers as it can before building new layers once 
the Dockerfile's instructions diverge from any existing stored layers.

When you need an image of something that isn't specific to your application, such as a Node instance, you 
could of course build it yourself from a base Linux image, but you could save yourself the trouble and 
see if the Docker Registry has an official image that would do the trick. Node.js provide an up-to-date
selection of images for public use, and all are named and tagged for your convenience. The example Dockerfile
pulls v0.10 of Node from the Docker registry like so:
```
FROM node:0.10
```

The MAINTAINER field simply records the contact details of the Dockerfile's creator:
```
MAINTAINER Sam Broughton <sam@example.com>
```
 
Docker version 1.6 added the ability to add metadata to Docker images through the LABEL command, which
can be handy when searching for particular Docker images and containers:
```
LABEL "rating"="Five Stars" "class"="First Class"
```
You can see the labels attached to an image by `inspect`ing it.

Docker runs all processes as the `root` user by default; you can change user with the `USER` command:
```
USER root
```
Keep in mind that allowing the Docker container to use the `root` user can potentially expose the host system.
Production containers should almost always be run using a non-privileged user.

You can set shell variables using the `ENV` command that can be accessed later in the build process. This allows
you to keep the Dockerfile __DRY (Don't Repeat Yourself)__, and can help to avoid typos, particularly when
referencing file paths:
```
ENV AP /data/app
ENV SCPATH /etc/supervisord/conf.d
```

You can execute shell commands with a `RUN` instruction. Here, we are updating the image's information about 
apt's packages, making sure they are up to date. Using the `-y` flag means that the comand will not ask for 
confirmation.
```
RUN apt-get -y update
```
Here we are installing the 'supervisor' package - Supervisor is a client/server system that allows its users to
monitor and control a number of processes on a UNIX-like OS.
```
RUN apt-get -y install supervisor
```
And here we are creating a directory for Supervisor's log output within the image's filesystem.
```
RUN mkdir -p /var/log/supervisor
```

Note that running a package update command like `apt-get update` or `yum update` is not generally considered to
be best practice, since it can lead to lengthy image build times. Consider basing your application image on an
image that has already run this command to avoid this issue.

Keep in mind that each Dockerfile instruction adds another file system layer, so it makes sense to group 
commands together into logical groups. You might even import lengthy commands from external files using 
the `ADD` command, and then execute them in your Dockerfile.

The `ADD` command copies files from the your local filesystem into the image, after which the image can be used
without having access to the original files, as they are now stored within the image itself. Usually these files
will include your application code as well as any required support files.
```
ADD ./supervisord/conf.d/* $SCPATH/
ADD *.js* $AP/
```

The `WORKDIR` command lets you change the current working directory the image uses for any subsequent 
instructions.
```
WORKDIR $AP

RUN npm install
```

Since Docker will rebuild the image starting with the first instruction that has changed, it is important to put
commands that will involve frequent changes, such as adding your local files, at the end of the Dockerfile. This
way Docker will have to rebuild less of the image after each change.

The final command in the Dockerfile is the `CMD` instruction, which specifies the command you wish to run in your
container once it has been created. Here, we are using Supervisor's `supervisord -n` command to run the node
application within the container and to ensure that it continues running.
```
CMD ["supervisord", "-n"]
```
Conventional wisdom argues that only one process should be run within each container, so that it is easy to 
horizontally scale individual functions within your architecture.

## Building an Image

This section will use the 'docker-node-hello' project files. Let's go through the files one by one.

Dockerfile: this is the project's Dockerfile.

.dockerignore: this file lets you specify files that you don't want uploaded to the Docker host while the image
is being built. Adding '.git' to the .dockerignore file saves image build time by excluding the git repository
files from the image build, since they are unnecessary in this context.

package.json: this defines the basics of the Node.js app, and specifies any dependencies required.

index.js: the source code for the app.

After navigating to the 'docker-node-hello' directory in your terminal, you can attempt to build an image from 
this project's Dockerfile with the following command (keep in mind that the first build will take far longer 
than subsequent builds, since the base image must be downloaded):
```
docker build -t example/docker-node-hello:latest .
```

To save time, Docker will often use a local cache during a build (you can see this in the Docker build output
wherever it says something like `---> Using cache` instead of `---> Running in 23671c2f57b7`. If you wish to 
disable the cache, you can add the `--no-cache` argument to the `build` command)

## Running Your Image

Once the image has been built, you can create a running container based on it using the `docker run` command
followed by `-d` to tell it to run in the background, `-p` to map a container port to a host port, and then 
specify the tag of the image you wish to use for the container:
```
docker run -d -p 8080:8080 example/docker-node-hello:latest
```
You can check that the container is running using `docker ps`, and may also visit http://localhost:8080 to
hopefully see the app's 'Hello World' message. 

### Environment Variables

The index.js file constructs the 'Hello World' message using the WHO variable, which can either use the default
DEFAULT_WHO value of 'World' or, as we will learn, it can use an environment variable passed to the container
when it is started.

First, stop the running container by using `docker ps`, copying the CONTAINER ID of the container you wish to
destroy, and then using `docker stop ???`, replacing '???' with the container id. Then restart the container
using the command we used earlier, adding one extra argument:
```
docker run -d -p 8080:8080 -e WHO="Sean and Karl" example/docker-node-hello:latest
```
If you reload your browser, the "Hello World' message should have been replaced by "Hello Sean and Karl".

### Custom Base Images

Base images are used as a starting point for your own image, and are usually based on bare-bones installs
of Linux distributions such as Ubuntu, Fedora or CentOS; they can however be much smaller still and contain
only a single __statically compiled binary.__ In most cases it is best to just use the official base image for
the distribution or tool you require for your service.

It might happen, however, that you want to have more control over your base image; such use cases include
wanting to maintain a consistent OS image across your own projects, or if you want to drastically reduce the
image size - if your application is simply a statically built C or Go project, then have an entire Linux
distribution as your base file is unnecessary. Similarly, you might find that you need fewer tools than 
the official distributions provide you with; fine-tuning your project in this way can lead to faster
deployment times and easier application distribution.

## Storing Images

Once you've created your image, the next step is to store it somewhere it can be accessed by any Docker host
on which you'll want to run a container. In most cases you won't want to build the image on the server itself
- you'll instead want to pull a pre-built image from a repository, so you'll need to store the image somewhere
using one of the following options.

### Public Registries

Docker has its own public registry for storing images, Docker Hub; other options are available, such as Quay.io.

Both Docker Hub and Quay.io enable you to store your images at a centralised, publicly accessible location,
and also allow you to store private images if you want to restrict who is able to use your image. Additionally,
both services have nice user interfaces, and offer reasonably-priced options for higher levels of storage.
If your organisation is getting started with Docker, but does not yet require a robust in-house solution, one
of these options will serve you well.

The major downside to using these services is that you will need to pull entire images over the internet, which
can greatly slow down deployment, and even restrict your ability to build at all in case of an internet outage.
This downside can be mitigated by taking care to design streamlined and compact images with small file sizes.

### Private Registries

Alternatively, you could host your own local private registry. Docker provides a 'docker-registry' tool for this
purpose, and can interact with the Docker client to enable you to push, pull and search for images.

Another solution is to use the CoreOS Enterprise Registry, which provides more or less the same features as
Quay.io, and can be deployed locally.

Docker has since released Docker Hub Enterprise, which provides organisations with an on-premise image registry
that can be stored in a data centre or cloud environment.

### Authenticating to a Registry

Many registries require authentication in order to interact with them. You can easily gain access to the Docker
hub registry by creating a Docker Hub account online, and then using `docker login` and entering your details.
Behind the scenes, Docker has created a directory at ~/.docker that stores all your credentials, and Docker
will check this directory to validate any attempt to interact with a registry that requires authentication.

### Mirroring a Registry

If you wish, you can set up a registry on your local network that will mirror images from an upstream public
registry so that hosts on your network won't need to pull images from the internet.

### Other Approaches to Image Delivery

There have been efforts by the community to provide even more approaches to image delivery. Dogestry, for
instance, can create and load images from cloud storage platforms such as Amazon S3. Other work such as 
'torrent-docker'has focused on a torrent-based solution, since torrents are capable of distributing files
to a group of servers simultaneously. The official Docker Distribution project aims to provide a command-line
tool for pushing and pulling Docker image layers without requiring a Docker daemon.

This said, you should always start by using the off-the-shelf solutions until your business requirements dictate
a different approach, as the tried-and-tested tools will serve almost everyone.