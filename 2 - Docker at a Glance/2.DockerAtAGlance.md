# Chapter 2 - Docker at a Glance

While Docker is a very powerful technology, it's not particularly complicated. What follows is a discussion
of how it works, what makes it great, and why you might want to use it when building your applications.

### Process Simplification

As mentioned in the previous chapter, Docker can take the place of much of the communication between development
and operations teams, vastly reducing the complexity of the path to production. In the past, this path has been
the following:
1) Developers request __resources__ from operations engineers.
2) Resources are __provisioned__ and supplied to devs.
3) Devs build the application.
4) Ops and devs tweak the app repeatedly.
5) Devs realise the app needs more dependencies.
6) Ops install the additional dependencies.
7) Steps 5 and 6 are repeated N times.
8) The app is eventually ready to be deployed.

Under this system it can typically take as much as a week to take an app into deployment even after it has been 
finished! A nasty side-effect of this lengthy and awkward process is that developers are disincentivised to
innovate, and will instead by reluctant to deploy anything other than major changes in the interest of saving
themselves a headache.

__Heroku__ is a popular __push-to-deploy__ system, which enables a developer to be in full control of their
dependencies and therefore the development cycle. Ops engineers are very familiar with the complaint that
their internal systems are far clunkier than Heroku.

Docker provides developers with even more control than systems like Heroku, since in addition to cleanly 
separating responsibilities and encapsulating dependencies, they are also able to define everything down 
to the OS distribution their application is shipped upon.

Docker subscribes to a "batteries included but removable" policy, meaning that they want to supply developers
with everything they need while at the same time allowing parts to be swapped out and replaced by custom
parts.

The use of image repositories is important, as it defines the dividing line between the application image
definition (the general and long-lasting) and the running of the container built on that image (the
specific and temporary).

Since Docker allows developers this increased level of control when creating the app, they are able 
to run it in both development and test environments, and then ship the same bundle to production, secure
in the knowledge that the app will continue to run in whatever environment to which it is deployed. Ops
engineers can then use whatever mechanism they decide on to deploy and run the application. The cycle then becomes
the following:
1) Developers are empowered to build the Docker image of the app without Ops input.
2) Ops take care of configuring the container and provision the necessary resources.
3) Devs trigger deployment.

Since all the necessary dependencies must be present at the development and test stages of the application,
there is nothing more for the ops team to do once the application is ready to be put in production. This means
that there is far less coordination required between the dev and ops teams, and also leads to better software
since the testing and deployment environments are guaranteed to be identical.

### Broad Support and Adoption

Docker is a very popular tool, and is well-supported by most of the large __public clouds;__ examples include
AWS, Elastic Beanstalk, Google AppEngine, IBM Cloud, Microsoft Azure and Rackspace Cloud. In 2014 Google even
announced that they would be using Docker as their __primary internal container format__. This is all very 
good news for Docker, as it fosters a strong community, and ensures efforts will continue to be made to 
make Docker a secure platform with great resources.

Further to this, cloud providers are adopting Docker as their primary container format. Docker's __libswarm__
development library allows for the orchestration of deploying a container to a mixture of different cloud
providers simultaneously, which had not been previously possible.

Despite requiring the use of the Linux kernel, Docker can now be run on most major operating systems.

### Architecture

At odds with Docker's power is its simplicity. Its basic architecture is simply a client/server model, and the
__docker__ executable enables an engineer to give commands to both. Beneath this streamlined interface, Docker
makes use of __kernel mechanisms__ such as __iptables,__ __virtual bridging,__ __cgroups,__ __namespaces,__ and
various __filesystem drivers,__ all of which we will cover in Chapter 10. For now, however, we will focus on the 
__client,__ the __server__ and the __network layer__ that a Docker container rests upon.

### Client/Server Model

Docker is made up of two primary parts - the client and the server (or daemon). There is also a third, optional,
part called the __registry,__ which is responsible for storing Docker images and those images' metadata. The Docker
daemon runs on any number of servers, and takes care of the running and management of your containers, while the
client is your interface for issuing commands to the servers.

Unusually for client/server software, Docker is packaged as a single executable, using the same __binary__ for
both components. In most cases Docker will launch one daemon per host, which can then manage multiple containers
and be issued commands via the client.

### Network Ports and Unix Sockets

The Docker client and daemon communicate via __network sockets,__ and the daemon can be instructed to listen
on multiple __Unix sockets__ or __TCP (Transmission Control Protocol) ports.__ On many Linux distributions it
is the default to have Docker listen on a local Unix socket and both an encrypted and non-encrypted TCP port.
This affects how you are able interact with the Docker daemon - if you only care about issues commands to it
from the host machine itself then all you need is the Unix socket, but if you want to do this remotely you'll 
need to use the TCP port(s).

Originally, Docker used TCP port 4243, but this was unofficial and overlapped with other existing tools - since
then, Docker has registered its own reserved TCP port with __IANA__ and now is usually configured to use 2375
for non-encrypted traffic, and 2376 for encrypted traffic. Docker uses 2376 by default post version 1.3 but can
easily be changed. The particular Unix socket used varies from OS to OS, but can also be configured if you care
to do so.

### Robust Tooling

Dockers basic tooling includes support for building Docker images, basic deployment to individual Docker daemons, 
and everything you need to remotely manage a Docker server. In addition to this, __Compose,__ __Machine,__ and 
__Swarm__ aim to account for every aspect of the development process by taking care of the management of
__clusters__ of servers as well as the scheduling and deployment of containers.

In addition to the __Command-Line Tool,__ Docker provides a remote web __API (Application Programming 
Interface),__ which allows users to add their own tooling in any language.

### Docker Command-Line Tool

Most people will interact with Docker using this tool, which is a Go program that is able to run on all common
operating systems. Some of the things you can do with the tool are the following:
- Building container images
- Pushing/Pulling Docker images to/from a registry from/to a daemon.
- Starting Docker containers in the foreground or background.
- Retrieving Docker logs from a remote server
- Start a command-line shell inside a running container on a remote server
Even though the tool most likely provides you with all the commands you'll need, you may also use the API,
which is arguable more powerful.

### Application Programming Interface (API)

Most modern pieces of software have a remote API, and the Docker daemon is no different; in fact, this is how
the CL tool communicates with the daemon. It's also possible, however, for external tooling to use this API 
for all sorts of advanced processes such as __mapping deployed Docker containers to servers,__ 
__automated deployments,__ __distributed schedulers.__ As your proficiency and the requirements of your 
application grow, you'll probably find yourself using the API more and more.
 
Documentation for the API can be found [here](https://docs.docker.com/develop/sdk/).

It's simple to perform any of the commands the CL tool can, except for running remote shells or executing a 
container in interactive mode, in which case it would be better to use the tool.

### Container Networking

As much as Docker containers may resemble Virtual Machines, they are in large part just processes running on the
host system itself; however, they behave differently from other processes at the __network layer.__ Containers 
behave on the network like hosts on a private network; the Docker server acts as a virtual __bridge__ with the
containers as clients behind it.

Each container has its own virtual __Ethernet__ interface, which is connected to the Docker bridge, as well as
its own allocated __IP address.__ Docker allows you to __bind__ ports on the host to the container, enabling the
outside world to reach it. Before reaching the container, the traffic passes over a __proxy__ which is also part
of the Docker daemon (More on this in Chapter 10).

Docker allocates the __private subnet__ from an unused __RFC 1918__ private subnet block. On startup it
automatically finds an unused network block and allocates one to the virtual network. This is then bridged to 
the host's __local network__ through an interface on the server called __docker0.__ As a result, all of the 
containers are on a network together, enabling direct communication between them, but in order to reach the 
outside world they must go over the docker0 virtual bridge interface. __Inbound traffic__ goes over the proxy
which, while high-performance, can limit your application in a high-throughput scenario (more on this in Chapter 
10).

Configuring Docker's network layer is a very broad topic, and can be done in many different ways, including
__allocating your own network block__ and __configuring your own custom bridge interface,__ but the default
setup will most likely be sufficient until you find that your application needs something more specific (more
on this in the Advanced Topics chapter).

You might as well keep the default networking configuration while you're getting to know Docker, but later on
you might realise that you don't need a virtual network at all; in this case you can disable it, and Docker
will use the host's own network devices and addresses.