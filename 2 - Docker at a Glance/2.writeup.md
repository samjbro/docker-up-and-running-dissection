# Chapter 2 - Docker at a Glance

While Docker is a very powerful technology, it's not particularly complicated. What follows is a discussion
of how it works, what makes it great, and why you might want to use it when building your applications.

### Process Simplification

As mentioned in the previous chapter, Docker can take the place of much of the communication between development
and operations teams, vastly reducing the complexity of the path to production. In the past, this path has been
the following:
1) Developers request __resources__ from operations engineers.
2) Resources are __provisioned__ and supplied to devs.
3) Devs build the application.
4) Ops and devs tweak the app repeatedly.
5) Devs realise the app needs more dependencies.
6) Ops install the additional dependencies.
7) Steps 5 and 6 are repeated N times.
8) The app is eventually ready to be deployed.

Under this system it can typically take as much as a week to take an app into deployment even after it has been 
finished! A nasty side-effect of this lengthy and awkward process is that developers are disincentivised to
innovate, and will instead be reluctant to deploy anything other than major changes in the interest of saving
themselves a headache.

__Heroku__ is a popular __push-to-deploy__ system, which enables a developer to be in full control of their
dependencies and therefore the development cycle. Ops engineers are very familiar with the complaint that
their internal systems are far clunkier than Heroku.

Docker provides developers with even more control than systems like Heroku, since in addition to cleanly 
separating responsibilities and encapsulating dependencies, they are also able to define everything down 
to the OS distribution their application is shipped upon.

Docker subscribes to a "batteries included but removable" policy, meaning that they want to supply developers
with everything they need while at the same time allowing parts to be swapped out and replaced by custom
parts.

The use of image repositories is important, as it defines the dividing line between the application image
definition (the general and long-lasting) and the running of the container built on that image (the
specific and temporary).

Since Docker allows developers this increased level of control when creating the app, they are able 
to run it in both development and test environments, and then ship the same bundle to production, secure
in the knowledge that the app will continue to run in whatever environment to which it is deployed. Ops
engineers can then use whatever mechanism they decide on to deploy and run the application. The cycle then becomes
the following:
1) Developers are empowered to build the Docker image of the app without Ops input.
2) Ops take care of configuring the container and provision the necessary resources.
3) Devs trigger deployment.

Since all the necessary dependencies must be present at the development and test stages of the application,
there is nothing more for the ops team to do once the application is ready to be put in production. This means
that there is far less coordination required between the dev and ops teams, and also leads to better software
since the testing and deployment environments are guaranteed to be identical.

### Broad Support and Adoption

Docker is a very popular tool, and is well-supported by most of the large __public clouds;__ examples include
AWS, Elastic Beanstalk, Google AppEngine, IBM Cloud, Microsoft Azure and Rackspace Cloud. In 2014 Google even
announced that they would be using Docker as their __primary internal container format__. This is all very 
good news for Docker, as it fosters a strong community, and ensures efforts will continue to be made to 
make Docker a secure platform with great resources.

Further to this, cloud providers are adopting Docker as their primary container format. Docker's __libswarm__
development library allows for the orchestration of deploying a container to a mixture of different cloud
providers simultaneously, which had not been previously possible.

Despite requiring the use of the Linux kernel, Docker can now be run on most major operating systems.

### Architecture

At odds with Docker's power is its simplicity. Its basic architecture is simply a client/server model, and the
__docker__ executable enables an engineer to give commands to both. Beneath this streamlined interface, Docker
makes use of __kernel mechanisms__ such as __iptables,__ __virtual bridging,__ __cgroups,__ __namespaces,__ and
various __filesystem drivers,__ all of which we will cover in Chapter 10. For now, however, we will focus on the 
__client,__ the __server__ and the __network layer__ that a Docker container rests upon.

### Client/Server Model

Docker is made up of two primary parts - the client and the server (or daemon). There is also a third, optional,
part called the __registry,__ which is responsible for storing Docker images and those images' metadata. The Docker
daemon runs on any number of servers, and takes care of the running and management of your containers, while the
client is your interface for issuing commands to the servers.

Unusually for client/server software, Docker is packaged as a single executable, using the same __binary__ for
both components. In most cases Docker will launch one daemon per host, which can then manage multiple containers
and be issued commands via the client.

### Network Ports and Unix Sockets

The Docker client and daemon communicate via __network sockets,__ and the daemon can be instructed to listen
on multiple __Unix sockets__ or __TCP (Transmission Control Protocol) ports.__ On many Linux distributions it
is the default to have Docker listen on a local Unix socket and both an encrypted and non-encrypted TCP port.
This affects how you are able to interact with the Docker daemon - if you only care about issues commands to it
from the host machine itself then all you need is the Unix socket, but if you want to do this remotely you'll 
need to use the TCP port(s).

Originally, Docker used TCP port 4243, but this was unofficial and overlapped with other existing tools - since
then, Docker has registered its own reserved TCP port with __IANA__ and now is usually configured to use 2375
for non-encrypted traffic, and 2376 for encrypted traffic. Docker uses 2376 by default post version 1.3 but can
easily be changed. The particular Unix socket used varies from OS to OS, but can also be configured if you care
to do so.

### Robust Tooling

Dockers basic tooling includes support for building Docker images, basic deployment to individual Docker daemons, 
and everything you need to remotely manage a Docker server. In addition to this, __Compose,__ __Machine,__ and 
__Swarm__ aim to account for every aspect of the development process by taking care of the management of
__clusters__ of servers as well as the scheduling and deployment of containers.

In addition to the __Command-Line Tool,__ Docker provides a remote web __API (Application Programming 
Interface),__ which allows users to add their own tooling in any language.

### Docker Command-Line Tool

Most people will interact with Docker using this tool, which is a Go program that is able to run on all common
operating systems. Some of the things you can do with the tool are the following:
- Building container images
- Pushing/Pulling Docker images to/from a registry from/to a daemon.
- Starting Docker containers in the foreground or background.
- Retrieving Docker logs from a remote server
- Start a command-line shell inside a running container on a remote server
Even though the tool most likely provides you with all the commands you'll need, you may also use the API,
which is arguable more powerful.

### Application Programming Interface (API)

Most modern pieces of software have a remote API, and the Docker daemon is no different; in fact, this is how
the CL tool communicates with the daemon. It's also possible, however, for external tooling to use this API 
for all sorts of advanced processes such as __mapping deployed Docker containers to servers,__ 
__automated deployments,__ __distributed schedulers.__ As your proficiency and the requirements of your 
application grow, you'll probably find yourself using the API more and more.
 
Documentation for the API can be found [here](https://docs.docker.com/develop/sdk/).

It's simple to perform any of the commands the CL tool can, except for running remote shells or executing a 
container in interactive mode, in which case it would be better to use the tool.

### Container Networking

As much as Docker containers may resemble Virtual Machines, they are in large part just processes running on the
host system itself; however, they behave differently from other processes at the __network layer.__ Containers 
behave on the network like hosts on a private network; the Docker server acts as a virtual __bridge__ with the
containers as clients behind it.

Each container has its own virtual __Ethernet__ interface, which is connected to the Docker bridge, as well as
its own allocated __IP address.__ Docker allows you to __bind__ ports on the host to the container, enabling the
outside world to reach it. Before reaching the container, the traffic passes over a __proxy__ which is also part
of the Docker daemon (More on this in Chapter 10).

Docker allocates the __private subnet__ from an unused __RFC 1918__ private subnet block. On startup it
automatically finds an unused network block and allocates one to the virtual network. This is then bridged to 
the host's __local network__ through an interface on the server called __docker0.__ As a result, all of the 
containers are on a network together, enabling direct communication between them, but in order to reach the 
outside world they must go over the docker0 virtual bridge interface. __Inbound traffic__ goes over the proxy
which, while high-performance, can limit your application in a high-throughput scenario (more on this in Chapter 
10).

Configuring Docker's network layer is a very broad topic, and can be done in many different ways, including
__allocating your own network block__ and __configuring your own custom bridge interface,__ but the default
setup will most likely be sufficient until you find that your application needs something more specific (more
on this in the Advanced Topics chapter).

You might as well keep the default networking configuration while you're getting to know Docker, but later on
you might realise that you don't need a virtual network at all; in this case you can disable it, and Docker
will use the host's own network devices and addresses.

### Getting the most from Docker

When using Docker, keep in mind that it isn't the best solution for every problem you'll come across. Know when
to use it, and when not to use it.

Docker is not built with stateful applications in mind - you should only really use it in cases where state is
either immutable or externalised into a data store, like a database or a cache. Good use cases include frontend
web applications, backend APIs and short-running tasks like those traditionally handled by cron.

Traditionally, most apps are stateful, which means that a running app must keep track of critical data using a
database, files, or internal memory in order to perform its function; if the app is restarted then this data 
will be lost. Stateless applications on the other hand contain on start-up all the information they need in 
order to perform.

A piece of advice would be to initially focus on building stateless Docker apps, and then to move on to more
complicated use cases. Another thing to keep in mind is that the community is always working to improve Docker's
functionality (in fact, they have made great strides in dockerising stateful applications in the few years since
this book was published).

### Containers are not Virtual Machines

It is important to remember that containers are NOT Virtual Machines - they are much, much more lightweight -
it is better to think of them as a wrapper around a single Unix process; just like normal processes, they 
are very temporary, and overhead is small for starting a new one.

The major difference between Virtual Machines and containers is their intended lifespan - VMs are supposed to
behave in the same way as physical servers, which means that they are often kept running for extended periods
of time; containers on the other hand are supposed to be ephemeral, and may only run a single task before
being destroyed.

### Containers Are Lightweight

While VMs can take up hundreds if not thousands of megabytes of disk space, a container created from an
existing image can be as small as 12 kilobytes. This is because a container is simply a reference to a
__layered filesystem image__ along with some metadata about the image.

Containers' tiny size and low overhead creation cost mean that it can be efficient to spin up a container
purely to run a single command, something that would never be practical when working with VMs.
 
### Towards an Immutable Infrastructure
 
It has always been very difficult to maintain a configuration infrastructure that is completely reliable
and will deploy in the same way every time, regardless of environment. Docker vastly simplifies this
process, and allows individual containers to be redeployed, ensuring that the need to update a particular
aspect of your application does affect the infrastructure of the app as a whole. If the server itself needs
an important update, the containers can simply be redeployed on the new server and destroyed on the old.

### Limited Isolation

While containers are isolated from each other, they are still at heart just wrappers around processes running
on your host system, and as such they will compete for resources. While this is sometimes what you want,
it pays to keep this in mind as you are designing your application. If you realise you need to limit 
resource allocation, this is possible within Docker, but is not the default behaviour as it would be on
a VM.

Another way in which container isolation is limited is in cases where they share some or all of the same 
filesystem layer references. If a shared image is updated then all containers that are
dependent upon that image will have to be recreated.

Any process running within a Docker container is running on the Docker server itself, which is itself running
on the same Linux kernel instance as the host's operating system. This is very different from a hypervisor 
running VMs, in which case each VM would be run on a different instance of an OS.
  
While it might be tempting to expose more of the host's resources to containers, e.g. sharing a file system
between containers as a means of storing state, it is inadvisable to do this for security reasons, as will
be discussed later in the book (is this still the case? This sounds like volumes).

Another thing to watch out for is that it is quite easy to accidentally give the container's root user access
to the host's system resources, files and processes, as containers use __UID 0__ to launch processes.

### Stateless Applications

Docker works very well with stateless applications such as those which externalise their data store in a 
database. It's also not a problem to run ephemeral __memcache instances__ inside a container; but what about
things like configuration files? Although this does not seem like much data, not including it drastically 
limits the app's portability and ease of deployment into different environments, while storing it within 
the application's codebase runs contrary to Docker's intended use.
 
The solution for this in most cases is to extract this configuration state into environment variables that
can be supplied when the container is being created. These variables can be things such as database names or
the hostnames of other service dependencies.
 
When developing a containerised application it is useful to keep trying to optimise it by reducing everything
to the absolute minimum essentials your app needs in order to run; a handy tip is to think of anything you
need to run in a __distributed__ way as a container, and to let this drive your design decisions. An example
of interesting design arising out of this concept is if you were to require a service that collects data then
processes and returns it, you could have many instances of the collection service running on many servers,
and have a separate service for aggregating these responses.

### Externalising State

We have said that Docker works best with stateless applications; however, we sometimes cannot avoid needing
to store state somewhere. In the case of configuration, environment variables do the trick, and Docker can
handle these natively; if you define them in your Dockerfile, Docker will store them in the metadata that 
comprises the container configuration, meaning that containers will start up with the same configuration
data each time.

Docker works perfectly well with applications that use databases to store their state, but the same is not
true for apps that need to store files. A container's internal filesystem is not suited to persisting files,
as it is very limited in space and performance, and will not persist the files if the container is destroyed.
If you absolutely must give your containers access to external files, it is advisable to store them in a 
centralised location such as Amazon S3, RiakCS, OpenStack Swift, a local block store, or by mounting iSCSI
disks inside the container; then you can give containers access to the shared filesystem regardless of where
the container is deployed.

It is strongly recommended, however, that you start by using Docker with applications that do not need 
persistent state, since any solution that allows you to do so introduces dependencies that prevent the 
application from being truly portable.

## The Docker Workflow

Docker wants you to work in a particular way that, if adopted, will lead to great benefits in your 
organisational structure; however, the workflow Docker encourages might feel strange and jarring at first.
Stick with it, though!

### Revision Control

Docker provides you with two forms of __revision control;__ one that is used to track the filesystem layers
that make up a Docker image, and the other is a tagging system for containers.
 
#### Filesystem Layers

A Docker image, and the containers built from an image, are constructed of __stacked filesystem layers,__ each
layer built upon another. Docker leverages this to enable much more efficient image building, since Docker will
be able to reuse any layers that preceded the change. It also makes images far more reusable, as new images with
the same starting points as existing images can use the same file layers, meaning that they don't have to be
built from scratch. There is also no chance of things having changed unexpectedly between these images being 
built, as each container has explicit dependencies.

In other words, Docker will use as many base layers as it can when building or rebuilding an image, resulting in
reduced storage space required and much faster build times.

#### Image Tags

It is important to keep track of the version of an application that has been deployed, and non-Dockerised
applications have several ways of achieving this, such as __git tags__ for each release, __deployment logs,__
__tagged builds__ for deployment, or using __Capistrano,__ which will handle this for you by storing a
number of previous releases on the server and using __symlinks__ to determine which is the current release.

A nasty side-effect of this heterogeneous mix of solutions is that there is no uniform solution to the problem -
your own codebase could have multiple systems for managing releases based on the language or version control
system that particular application uses. Docker provides a uniform solution in the form of __image tagging__ at
deployment time. Similarly to the Capistrano solution, you can then leave multiple builds of your application
on the server and tag them to determine which is the current release; Docker's image tagging functionality
is more useful, however, since it makes it trivial to standardise this practice across all your Dockerised
applications.

It is common for Docker images to have the __latest__ tag applied, which result in that image being supplied
whenever the image is requested without a specified tag; however, this should be used carefully, as the
build tagged as __latest__ will vary, and mean that images with the same dependency will build differently
depending on external factors, which is a big no-no for a dockerised application.

### Building

Taking an application from development to a __well-formed, shippable artifact__ can be an opaque and 
unintuitive process, and is responsible for a lot of the overhead involved in deploying a new application.
While Docker doesn't make this process trivial, it does standardise it and provide a methodology that should
improve this potentially painful aspect of application development.

The Docker CL tool has a `build` command which __consumes__ a __Dockerfile__ to produce a new Docker image.
Each individual command in the Dockerfile creates a new file system layer building-block for the image, 
making it very easy to see how an image is constructed based on its Dockerfile. By standardising this build
process in the form of building Dockerfiles, any Dockerised application is built using standardised artifacts;
it is then far easier for any engineer to look under the hood of any build regardless of the language or even
the OS the app is based on.

As a result of this standardised build process, we can create build pipelines in systems like Jenkins so that
building an application can be made trivial and/or automated. Some companies such as eBay have even created
standardised Docker containers capable of building images from Dockerfiles to streamline this process even
further.

### Testing

Docker does not provide users with tools for testing, but the structure of a Dockerised app makes it easier
to use external tools to achieve the same end.

Docker's great boon to testing is that it leaves no room for doubt that the tested artifact is identical to
the the one that will be shipped to production, since we can use a custom tag or Docker's SHA-generated hash.
Furthermore, we can be sure that app will not run into unexpected dependency issues at deployment time, since
the tested container will contain identical dependencies to the shipped container. The very server that the
container is running on will be captured by the scope of the test, meaning that we can run true end-to-end
tests.

Another testing-adjacent benefit to a Dockerised app is that it greatly simplifies the development process
when an app is required to communicate with one or more other applications via API; in those cases an
engineer can simply develop against a tagged version of the other service without having to understand
anything about it. This is invaluable when working within systems of interconnected micro-services.

### Packaging

The packaged artifact that Docker produces is __uniform,__ regardless of the language your application
is written in or the OS distribution you run it on. Docker's name comes from an apt metaphor for the way 
in which Docker works; shipping containers are standardised, uniform, transportable units that can be
transported and handled regardless of the contents. The way that Docker has standardised the industry's
approach to application deployment opens the door for far more efficient and intuitive ways of reusing
tools between applications, or of slotting third-party services into your application without cumbersome
configuration.

### Deploying

Traditional tools for handling deployment include Capistrano, Ansible, Fabric, shell scripting, and in-house
custom tooling, any of which requires specialist knowledge, resulting in teams that are dependent upon
individuals to get a deployment to work. Docker removes this dependency altogether by providing a simple,
standardised strategy for getting deployments up and running. While the Docker client itself only enables
deployment to a single host at a time, there are tools that enable deployment to an entire cluster of hosts
simultaneously, without complex configuration knowledge required.

## The Docker Ecosystem

Docker has attracted a large community of both developers and __system administrators,__ which has stimulated
the development of better tools for solving operations problems. Where Docker itself does not provide the 
desired functionality, companies and individuals have created tools to plug the gaps, many of which are
open source.

### Orchestration

Coordinating Docker containers on a medium or large scale is non-trivial, and tools have been created to
enable orchestration and mass deployment. Tools for simpler use cases include __Docker Swarm,__ __New Relic's
Centurion__ and __Spotify's Helios,__ while more complex environments are covered by __Google's Kubernetes__
and __Apache Mesos.__ Keep an eye out for new tools, as the community is constantly working to provide an
improved experience.

### Atomic Hosts

Traditionally an organisation will have to carefully construct and maintain a network of servers and virtual
machines upon which to deploy their applications, and any updates and patches will most likely be applied
without bringing the system down, which can lead to all sorts of unexpected bugs and quirks. Software
deployments are usually deployed as a whole rather than an attempt even being made to patch them while
they are running.

Projects like __CoreOS__ and __Project Atomic__ have tried to apply the container-based philosophy to this
process, and aim to extend Docker's ability to tear down and redeploy applications to the entire software
stack, right down to OS components, and in so doing provide a pattern that is far more consistent and 
reliable.

The focuses of an [Atomic Host](https://gist.github.com/jzb/0f336c6f23a0ba145b0a) are a __minimal footprint,__
support for Linux containers and Docker, and the functionality to enable __atomic__ OS updates and rollbacks.
They aim to be easily controllable via __multihost orchestration tools__ on both __bare metal__ and __common
virtualisation__ platforms.

Chapter 3 will delve into how to use atomic hosts in the development process. Using atomic hosts as __deployment
targets__ results in exceptional __software stack symmetry__ between development and production environments.

### Additional Tools

Other categories of tool include __auditing,__ __logging,__ __network,__ __mapping,__ and many more.

## Wrap-Up

So concludes our overview of Docker. Later in the book we'll examine Docker's architecture in more detail,
as well as providing examples of how you might use the community tooling and an examination of how you
might design __robust container platforms.__