# Chapter 5 - Working with Docker Containers

Now that we are familiar with how to build an image, let's take a closer look at the containers created from those
images and how to control and configure them.

## What Are Containers?

Virtualisation systems such as VMware and Xen enable you to run a complete Linux kernel and OS on top of a 
virtualised layer called a __hypervisor.__ This technique results in a high level of isolation between virtual
machines, since each hosted kernel sits in separate __memory space__ and has __defined entry points__ into the
host's hardware.

__Containers__ take a very different approach, where all containers share the same instance of the Linux kernel,
and any isolation is enforced entirely within that kernel; this approach is called __operating system
virtualisation.__ A succinct definition of a container is as follows: "A container is a self-contained execution
environment that shares the kernel of the host system and which is (optionally) isolated from other containers
in the system". The most obvious advantage containers have over virtual machines is that they don't require 
anywhere near the resources the latter does. Sharing a kernel means that there is one fewer __layers of 
indirection__ between the isolated task (the container) and the hardware it is running on. Rather than entire
second kernel being instantiated, there is instead a little shim placed inside the existing kernel.

A side-effect of this approach is that you can only run Linux-based processes within a container, as they must
be compatible with the underlying kernel; contrast this with virtual machines, which can construct an environment
on top of whatever kernel is required. For this reason it is very important not to conflate containers with 
virtual machines, and to instead think of them as simply being wrappers around processes that are actually running
on the server.

## History of Containers

Much of the advancements that make Docker possible have been developed over the last 30 years, in several different
areas; a __system call__ added to the Unix kernel in the late 1970s and tooling built on modern Linux are key
foundations that Docker is built upon.
 
The idea of isolating and encapsulating part of a running system is an old idea, dating back to the very first
__batch processing systems.__ These would run one program, then switch to run another, making sure that the two
were isolated from one another in order to avoid unintended interaction.

The first true step towards making a modern container was the `chroot` system introduced to Version 7 Unix in 
1979, which is used to restrict the parts of the underlying filesystem that a process has access to. It is 
commonly used to guard the OS against publicly exposed and therefore untrusted server processes like FTP, 
BIND and Sendmail.

Over the next two decades, many Unix variants were created to have multiple tightly controlled domains running on
the same Unix kernel; processes run in one of these domains would have its view of the host's filesystem severely
limited so that there was no chance of processes interacting with processes on different domains. While most
mainstream Unix implementations were not capable of implementing this idea, the Sidewinder
firewall version of Unix, built on top of BSDI Unix, was a popular implementation that did so.

When FreeBSD 4.0 was released in the year 2000, it included a new `jail` command, which enabled 
__shared-environment hosting providers__ to segregate their processes from those of their customers by 
expanding upon `chroot`'scapabilities and enabling processes to be restricted within 'jails', which rendered 
them incapable of interacting with either the underlying file system or processes in other jails.
 
The first major commercial use of container-based technology arrived in 2004 when Sun released an early build of
Solaris 10, which is still used commercially by many to this day; in 2007, HP released __Secure Resource 
Partitions,__ later renamed as __HP-UX Containers.__ Finally, Linux joined the game in 2008 with __Linux Containers 
(LXC)__ which were introduced as part of version 2.6.24 of the Linux kernel; however, it wasn't until 2013 the
community began to gather around Linux Containers, when user namespaces were added to version 3.8 of the Kernel, 
and Docker was released a month later.

The exponential growth of the internet caused major companies such a Google to seek solutions for scaling their
applications, and many became early adopters of container technology in order to distribute their services across
entire data centres full of computers as a means of keeping up with demand. As the tech community started to realise
the need for container-based technology, many companies started to develop in-house solutions, while Google
contributed to the Linux kernel itself in an attempt to improve the tools available in this area.

Eventually the Linux Container frenzy boiled over, with the open source version of Google's container-based
solution, __lmctfy,__ being released only a few months after Docker was unveiled to the world; additional 
alternatives also sprang up, notably __CoreOS Rocket,__ which has the stated intention of rectifying what some
argue are serious deficiencies in the approach Docker takes to containerisation.

## Creating a Container

While in the previous chapter we used the `docker run` command to spin up a container from an image, this is really
a convenience command which first creates the container from the image, and the executes it - these steps can be
achieved individually using the `docker create` and `docker start` commands.

Any configuration options you wish to apply to container can be added to the `docker run` or `docker create` 
commands, such as the `-p` argument to link a host port to a port on the container, or `-e` to specify environment
variables. Below are some more configuration options you might use.

### Basic Configuration

While you can specify many of these settings in your Dockerfile, you can override them by passing arguments to the
command you use to create your container.

#### Container name

You can name your container using the `--name` argument. If you omit this argument, Docker will name your container
for you using a random adjective followed by a random famous coder's name. Each container must have a name unique
to that Docker host.

#### Labels

As we explored in Chapter 4, you may specify labels in your Dockerfile to provide an image with searchable 
key-value pairs as metadata; you may also add new labels at image creation using `labels -l key=value -l name=Sam`, 
etc. and then filter containers using `docker ps -a -f label=name=Sam`. Using `docker inspect` will also include
any labels a container in the resulting readout.

#### Hostname

Docker's default behaviour is to copy certain system files from the host to the container when it is created, such
as `/etc/hostname` to the wherever the host's configuration directory is for that container; it then uses a __bind
mount__ to link that copied file into the container itself.

For demonstration purposes, we'll `run` a container using the `--rm` argument, telling it to delete the container
once it is exited, and the `-i` and `-t` arguments for respectively starting an interactive session and allocating
 a __pseudo-TTY.__ Finally we'll specify that we want to run a `/bin/bash` command so that we can open a bash
session within the container. Put together, the command is as follows.
```
docker run --rm -it ubunt:latest /bin/bash
```
Once the container is running, you can use the `mount` command to see a list of all the bind mounts that have been
mounted to the container. When you use a bind mount, a file or directory from the host machine is mounted into the
container; since it does this by referencing the filepath location on the host machine, this technique relies upon 
the Docker host having a specific directory structure, making the container less portable. Modern Docker apps tend 
to use named volumes instead of bind mounts. The bind mount we are interested in looks like this:
```
/dev/sda1 on /etc/hostname type ext4 (rw,relatime,data=ordered)
```
This shows us that Docker has linked the container's /etc/hostname to the hostname file Docker has placed on the 
host machine, which by default is set to the container's randomly generated ID. We can assign a hostname ourselves
using the `--hostname="your-hostname-here""` argument, and we can check the hostname by running `hostname -f`
inside the container.

#### Domain Name Service (DNS)

The host's resolv.conf file is copied by default into the container's /etc folder and managed via a bind mount in 
the same way as the /etc/hostname file; we can override this behaviour using the `--dns=` and `--dns-search=`
commands to respectively specify 'nameserver' and 'search' fields.
 
#### Media Access Control (MAC) address

By default, a container will receive a __calculated MAC address__ that starts with the __02:42:ac:11__ prefix.
While you are unlikely to want to modify this behaviour, you can add a `--mac-address=` argument in case, for 
instance, you want to reserve a particular set of MAC addresses for your containers in order to avoid conflict
with other virtualisation layers on the same __private block__ as Docker. Be very careful if you choose to do 
this as you may cause __ARP contention__ if two systems advertise the same MAC address.

#### Storage Volumes

While it is not generally advisable to mount storage from a Docker host to container since the container is then
dependent upon being deployed on a particular host in order to function, there are cases in which it makes sense
to do so, such as for __temporary cache files__ or other __semi-ephemeral__ states.
 
In these cases we can use the `-v` command to mount a filesystem on the host server to the container as a 
__volume__ as follows:
```
docker run --rm -it -v /mnt/session_data:/data ubuntu:latest /bin/bash
```
Here, we are mounting the filesystem found at /mnt/session_data on the host system to the /data directory in the
container. Volumes get mounted with read-write privileges, and the mount point in the container does not need to 
already exist.

The outcome of mounting a volume is that any files added, deleted or changed within the container's specified
directory will also occur in the specified host's directory, enabling you to persist files on the host system
after the container has been destroyed.

Docker 1.5 added a `--read-only=true` to allow us to prevent the container's __root volume__ from writing anything
the host's root filesystem, for cases in which logfiles can unexpectedly fill up a container's allocated disk in
production. Mounted volumes can then be used to ensure that the container is only able to write data to the 
specified locations.

Keep in mind that engineers should make every effort to create stateless containers wherever possible, as any 
persistent storage management makes a container much more difficult to deploy.
 
### Resource Quotas

When working in the cloud, a major issue to watch out for is referred to as the "noisy neighbour" problem; when you
have multiple applications running on the same physical system, the performance and resource availability of an
individual application may suffer as a result.

When booting up a virtual machine, it is trivial to specify exactly how much memory, CPU and other resources you 
wish that VM to have access to; Docker, on the other hand, is less straightforward. Under the hood, we need to
make use of the Linux kernel's `cgroup` functionality in order to configure the resources available to a Docker
container. 

We can achieve this through Docker's `create` command; any constraints applied at creation will remain for the
entire lifespan of the container, unless you manipulate the kernel cgroups directly under the /sys filesystem.

In order for this to work, however, your kernel must have these capabilities enabled; to check if your kernel
is already configured in this way, run `docker info` and check for any 'WARNING' messages at the bottom of the 
readout. If none appear, then you already have the necessary support enabled. If not, consult the Docker 
documentation for instructions on enabling this feature in your particular OS distribution.

#### CPU Shares

The __Central Processing Unit (CPU)__ power of a system is its computing power. Docker looks at the total
computing power of all the __CPU cores__ in a system, and splits this into 1024 parts, which it calls __cpu 
shares.__ It is then able to allocation some or all of these shares to a container; this will determine how much
processing time the container's process will be allocated, and therefore how much computing power it is allowed to
use. For example, say you have three containers and allocate to one the full 1024 shares, and 512 to each of the 
others; the __scheduler__ will schedule all three containers to run the same number of times, but the container
with 1024 allocated shares will be allowed to run for twice as long as either of the other two.

To illustrate this, we'll make use of Docker's `stress` command, which can be used to test the limits of a system.
The following command will use the 'progrium/stress' image (which contains the `stress` command) to create a 
container, using the `--cpu 2` argument to create two CPU-bound processes, `--io 1` to create one I/O-bound
process, and `--vm 2 --vm-bytes 128M` to create two memory allocation processes, altogether creating a __load
average__ of around 5. We then use the `--timeout 120s` argument to tell the container to run for 2 minutes before
shutting down.
```
docker run --rm -it progrium/stress --cpu 2 --io 1 --vm 2 --vm-bytes 128M --timeout 120s
```

We can run the `top` command in a separate terminal to see how the system is affected by the container's processes.

To demonstrate the power of Docker's use of cpu shares, we can run the same command with a `-c 512` argument added,
to limit the number of shares allocated to this container to 512, effectively halving the time for which the 
scheduler will allow the processes to run.
```
docker run --rm -it -c 512 progrium/stress --cpu 2 --io 1 --vm 2 --vm-bytes 128M --timeout 120s
```

While the output of `top` might look the same as before if your system is not very busy, the effect will be
noticeable when your processes are trying to compete for CPU resources. 

Docker's cgroup-based constraints are not the same as a VM's resource constraints; while the latter are hard limits,
a cgroup constraint is a __priority-based__ system, relative to the current amount of competition for system 
resources, and will be ignored in the case that extra resources are available.

#### CPU Pinning

A system may contain multiple CPU cores, and Docker is able to 'pin' a container to one or more of these cores, 
meaning that any work pertaining to this container will only be allowed to run on the specified cores.

The following command is again running our stress-test container, only this time using the `--cpuset-cpus=0` 
command, which will tell the container that it may only use the systems first (0-indexed) CPU core. You may specify
multiple CPU cores as follows `--cpuset=0,1,2`. You'll receive a 'Cannot start container' error if you specify a
core that doesn't exist.
```
docker run --rm -it -c 512 --cpuset=0 progrium/stress --cpu 2 --io 1 --vm 2 --vm-bytes 128M --timeout 120s
```

The `top` readout will show a lower percentage of CPU time spent in __user space,__ since we limited two CPU-bound
processes to a single CPU. Also note that any other CPU sharing restrictions will now be relative only to other
processes running on the same CPU core.

#### Memory

We can also specify the amount of memory a container has access to, though this differs from CPU limits in that
while CPU limits are _relative,_ memory limits are _constant._ If you have constrained the amount of memory that
a container may make use of to, say, 24GB, it will never be able to use more than that 24GB of memory even if the
system has more memory available. Linux's architecture actually makes it possible to assign more memory to a
container than the system has available in RAM, in which case the container will resort to using `swap`, just as 
a normal Linux process would.

We can impose a memory limit using the `-m 512m` argument, which will constrain the container to 512 MB of RAM and
512 MB of additional __swap space.__ The units this argument accepts are b, k, m and g, representing bytes, 
kilobytes, megabytes and gigabytes.

By default this command sets an memory swap limit identical to the memory limit, but we can specify the former
separately using the `--memory-swap` argument, which accepts the same units as `-m`. Setting a memory swap of -1
will disable the swap altogether for that container.

A benefit to having a hard memory limit rather than a relative one is that we won't experience a rapid shift in
container operation when a new container is introduced to the system; we must take care, though, to choose a
memory limit that will be sufficient for our requirements, since there is no room for maneuver once it has been set.

If a container exceeds its memory limit, the __Linux Out of Memory (OOM)__ killer will begin to kill processes in
the `cgroup` in order to reclaim memory, and since the container has only one running process, it will immediately
be shut down. We can test this using the following command.
```
docker run --rm -it -m 100m --memory-swap=100m progrium/stress --cpu 2 --io 1 --vm 2 --vm-bytes 128M --timeout 120s
```

#### ulimits

Another approach we might take when limiting resources on a Unix system is with __user limits.__ You can run 
`ulimit -a` to see a list of all the types of things for which limits can be set with the `ulimit` command.

Originally, all containers inherited the ulimits of the Docker daemon itself; this was unsatisfactory, 
however, since the dameon almost always requires a greater allocation of resources than any individual container 
would. Since the release of Docker 1.6 it is now possible to set the default ulimits of any container by
configuring the Docker daemon. The following command demonstrates constraining containers by default to hard
limits of 150 open files and 20 processes.
```
sudo docker -d --default-ulimit nofile=50:150 --default-ulimit nproc=10:20
```
These defaults can be overridden on a container-by-container basis using the `--ulimit` argument when `run`ning or
`start`ing a container.

## Starting a Container

Once we've `create`d our container, we can start it with the `docker start` command.
 
As an example, let's try running a container based on a Redis image. First we'll create the container:
```
docker create -p 6379:6379 redis:2.8
```
Then we can copy the full hash from the last line of the `create` output, or run `docker ps -a` and copy the short
hash from the CONTAINER ID column, and start the container using `docker start [hash]`. We can now verify that the
container is running using `docker ps`.

## Auto-Restarting a Container

While some containers are ephemeral by design, and are only intended to run a quick process before destroying
themselves, we may want a container to stay running once we've started it. This situation is particularly common
in production environments.

To achieve this we can use the `--restart` argument when `run`ning or `create`ing a container. The three options
we can pass with this argument are as follows. `no` is the default option, and dictates that the container will
_never_ restart after it has been exited. `always` ensures that the container will _always_ restart after exiting.
`on-failure:#` will restart the container only in the event of a non-zero exit code (i.e. if the container is
exiting due to some error). The # is replaced by a number signifying the number of times Docker will attempt to
restart the container before giving up.

If we replace the `--rm` flag in our failing stress test command above with `--restart=on-failure:3`, then 
immediately check `docker ps` we can see that Docker is attempting to restart the container, as expected when
encountering a non-zero exit code. It will continue to fail until it has attempted to restart three times, at
which point it will no longer appear in the `docker ps` output.

## Stopping a Container

Running the `docker stop` command does not simply pause the process as you might expect; the process is exited,
and will no longer appear in the normal `docker ps` output, and Docker will not attempt to restart it when
rebooting.

Once a container has been stopped, it will continue to appear in the `docker ps -a` output unless it has been
removed through use of the `--rm` flag or the `docker rm` command. We can therefore once again copy the container
id from this output and use it to start the container. Keep in mind that any memory contents the container may
have had when it was running will have been lost, though configuration details specified upon the container's
creation will have been saved.

Under the hood, we're sending our containers the same Unix signals that would be used to issue commands to any 
other process; in the case of `docker stop` we are sending the container a __SIGTERM__ signal and then waiting
for the container to __exit gracefully.__ Containers are governed by the same __process group signal propagation__ 
rules that apply to any other Linux process group. 

If you want to force a container to be killed if it hasn't exited gracefully within a certain time limit, you can
add the `-t [seconds]` argument to the `docker stop` command; this will still send the process a SIGTERM signal, 
but will now also send a __SIGKILL__ signal after the specified time if the container is still running. While it
is better to rely on the SIGTERM signal in most cases, it may occasionally be necessary to issue the SIGKILL 
instead.

## Killing a Container

Sometimes you may wish to skip the SIGTERM attempt altogether and issue a SIGKILL command right away; in these
cases we can use `docker kill [container id]`. We can then re`start` a killed container in the same way as we
might a `stop`ped one.
 
We can also use the `kill` command to send any other Unix signal to a container, much in the same way as the
Linux `kill` command can to a process. For instance we can issue a __USR1__ signal, telling the container to
execute some user-defined task with the following command.
```
docker kill --signal=USR1 [container id]
```
Alternatively we could send a __HUP__ signal, which is the signal sent when the terminal closes on a foreground
process (Signal Hang Up).

## Pausing and Unpausing a Container

Rather than stopping our container, we might sometimes want to keep it running but render it inactive for a time,
perhaps so that we can take a snapshot of its filesystem to use in the creation of a new image, or to free up
some CPU for some other task; for this, we have the `docker pause` command.

This command works by making use of the __cgroups freezer,__ which will prevent a process from being scheduled
while it remains frozen; in the case of a container, this will stop all container activity while preserving
its state, including memory contents. No Unix signal is sent to the process when doing so, so the container is
unaware that it has been frozen.

Checking `docker ps` after pausing a container will show that the container STATUS is still 'up', though it is
also now '(paused)'. While it is in this state any attempt to use the container will fail, as the scheduler is
not permitted the process to run.

We can unfreeze the container using the `docker unpause` command, after which the container will continue to run
as if nothing happened.

## Cleaning Up Containers and Images

Using Docker will inevitably lead to many inactive and redundant image layers and container folders cluttering up
our system. An inactive container can be removed with `docker rm [container id]`, while images that are not
currently being used by a container can be removed with `docker rmi [image id]`.

If you wish to remove a batch of containers or images in one go, the following commands will be of use.
`docker rm $(docker ps -aq)` will remove all inactive containers, and `docker rmi $(docker images -q)` will
remove all images not in use. Alternatively, you can use filters to determine which images or containers to remove;
`docker rm $(docker ps -aq --filter 'exited!=0')` will remove all containers that exited with a nonzero state,
and `docker rmi $(docker images -q -f "dangling=true")` will do the same for all images without tags. You are
encouraged to make your own filters as necessary using pipes (|) and other techniques. 

